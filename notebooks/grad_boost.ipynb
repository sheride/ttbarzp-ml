{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unedited 47-dim data\n",
    "\n",
    "Initial training run-through: didn't go long enough, poor results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 350$ GeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.5957           71.48m\n",
      "         2           0.4411           70.42m\n",
      "         3           0.3686           68.21m\n",
      "         4           0.3194           66.41m\n",
      "         5           0.2932           64.68m\n",
      "         6           0.2807           63.01m\n",
      "         7           0.2674           61.40m\n",
      "         8           0.2561           59.97m\n",
      "         9           0.2495           58.47m\n",
      "        10           0.2461           56.95m\n",
      "        20           0.2103           42.56m\n",
      "        30           0.1997           28.33m\n",
      "        40           0.1918           14.16m\n",
      "        50           0.1863            0.00s\n"
     ]
    }
   ],
   "source": [
    "data50_350Gm_train, data50_350Gm_test = DA.gettraintest(0)\n",
    "gradboost_350Gm = sigbg_model(\n",
    "    make_pipeline(StandardScaler(), GradientBoostingClassifier(n_estimators=50, max_depth=7, learning_rate=1, verbose=1)), \n",
    "    data50_350Gm_train, data50_350Gm_test);\n",
    "gradboost_350Gm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.965; FPR: 0.034\n",
      "Signal Significance @ L = 3000 fb^-1: 37.056\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 704.99 GeV\n",
      "Maximal significance of 57.286 @ threshold = 0.975\n",
      "(sig. maximized with tpr = 0.772, fpr = 0.008)\n",
      "(new mass sensitivity of 814.82 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(gradboost_350Gm, 350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 500$ GeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.4478           68.88m\n",
      "         2           0.2957           67.04m\n",
      "         3           0.2330           65.72m\n",
      "         4           0.2001           64.47m\n",
      "         5           0.1800           63.25m\n",
      "         6           0.1657           61.89m\n",
      "         7           0.1599           60.53m\n",
      "         8           0.1527           59.17m\n",
      "         9           0.1483           57.83m\n",
      "        10           0.1437           56.47m\n",
      "        20           0.1251           42.47m\n",
      "        30           0.1185           28.44m\n",
      "        40           0.1142           14.21m\n",
      "        50           0.1141            0.00s\n"
     ]
    }
   ],
   "source": [
    "data50_500Gm_train, data50_500Gm_test = DA.gettraintest(1)\n",
    "gradboost_500Gm = sigbg_model(\n",
    "    make_pipeline(StandardScaler(), GradientBoostingClassifier(n_estimators=50, max_depth=7, learning_rate=1, verbose=1)), \n",
    "    data50_500Gm_train, data50_500Gm_test);\n",
    "gradboost_500Gm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.979; FPR: 0.021\n",
      "Signal Significance @ L = 3000 fb^-1: 18.355\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 761.37 GeV\n",
      "Maximal significance of 29.919 @ threshold = 0.9754\n",
      "(sig. maximized with tpr = 0.878, fpr = 0.006)\n",
      "(new mass sensitivity of 884.84 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(gradboost_500Gm, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 1$ TeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2971           68.75m\n",
      "         2           0.1358           67.62m\n",
      "         3           0.0825           66.43m\n",
      "         4           0.0630           65.12m\n",
      "         5           0.0541           63.72m\n",
      "         6           0.0494           62.38m\n",
      "         7           0.0467           61.01m\n",
      "         8           0.0448           59.59m\n",
      "         9           0.0427           58.14m\n",
      "        10           0.0417           56.78m\n",
      "        20           0.0362           42.64m\n",
      "        30           0.0364           28.42m\n",
      "        40           0.0509           14.22m\n",
      "        50           5.5445            0.00s\n"
     ]
    }
   ],
   "source": [
    "data50_1Tm_train, data50_1Tm_test = DA.gettraintest(2)\n",
    "gradboost_1Tm = sigbg_model(\n",
    "    make_pipeline(StandardScaler(), GradientBoostingClassifier(n_estimators=50, max_depth=7, learning_rate=1, verbose=1)), \n",
    "    data50_1Tm_train, data50_1Tm_test);\n",
    "gradboost_1Tm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.993; FPR: 0.007\n",
      "Signal Significance @ L = 3000 fb^-1: 3.373\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 895.94 GeV\n",
      "Maximal significance of 6.201 @ threshold = 0.9752\n",
      "(sig. maximized with tpr = 0.977, fpr = 0.002)\n",
      "(new mass sensitivity of 1060.74 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(gradboost_1Tm, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 2$ TeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2686           69.35m\n",
      "         2           0.1030           68.18m\n",
      "         3           0.0500           66.84m\n",
      "         4           0.0318           65.29m\n",
      "         5           0.0238           63.94m\n",
      "         6           0.0214           62.56m\n",
      "         7           0.0203           61.15m\n",
      "         8           0.0201           59.75m\n",
      "         9           0.0181           58.33m\n",
      "        10           0.0188           56.95m\n",
      "        20           0.6980           42.77m\n",
      "        30           0.6984           28.54m\n",
      "        40           0.6986           14.28m\n",
      "        50           0.9584            0.00s\n"
     ]
    }
   ],
   "source": [
    "data50_2Tm_train, data50_2Tm_test = DA.gettraintest(3)\n",
    "gradboost_2Tm = sigbg_model(\n",
    "    make_pipeline(StandardScaler(), GradientBoostingClassifier(n_estimators=50, max_depth=7, learning_rate=1, verbose=1)), \n",
    "    data50_2Tm_train, data50_2Tm_test);\n",
    "gradboost_2Tm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.996; FPR: 0.004\n",
      "Signal Significance @ L = 3000 fb^-1: 0.232\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 968.87 GeV\n",
      "Maximal significance of 0.708 @ threshold = 0.974\n",
      "(sig. maximized with tpr = 0.993, fpr = 0.0)\n",
      "(new mass sensitivity of 1291.19 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(gradboost_2Tm, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 4$ TeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2686           69.49m\n",
      "         2           0.1030           67.89m\n",
      "         3           0.0500           66.53m\n",
      "         4           0.0318           65.13m\n",
      "         5           0.0240           63.81m\n",
      "         6           0.0217           62.51m\n",
      "         7           0.0201           61.12m\n",
      "         8           0.0187           59.76m\n",
      "         9           0.0181           58.34m\n",
      "        10           0.0171           56.97m\n",
      "        20           0.1860           42.89m\n",
      "        30           0.1938           28.64m\n",
      "        40           0.2203           14.31m\n",
      "        50           0.2260            0.00s\n"
     ]
    }
   ],
   "source": [
    "data50_4Tm_train, data50_4Tm_test = DA.gettraintest(4) # accidentally had this at index = 3 initially\n",
    "gradboost_4Tm = sigbg_model(\n",
    "    make_pipeline(StandardScaler(), GradientBoostingClassifier(n_estimators=50, max_depth=7, learning_rate=1, verbose=1)), \n",
    "    data50_4Tm_train, data50_4Tm_test);\n",
    "gradboost_4Tm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.997; FPR: 0.003\n",
      "Signal Significance @ L = 3000 fb^-1: 0.005\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 1007.64 GeV\n",
      "Maximal significance of 0.01 @ threshold = 0.9743\n",
      "(sig. maximized with tpr = 0.994, fpr = 0.001)\n",
      "(new mass sensitivity of 1222.76 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(gradboost_4Tm, 4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unedited 47-dim data Pt 2\n",
    "\n",
    "Trained longer, used XGBClassifier which is much faster than pure scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 350$ GeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/xgboost/data.py:106: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:09:52] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "data50_350Gm_train, data50_350Gm_test = DA.gettraintest(0)\n",
    "xgradboost_350Gm = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, nthread=4, use_label_encoder=False)), \n",
    "    data50_350Gm_train, data50_350Gm_test);\n",
    "xgradboost_350Gm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.975; FPR: 0.0434032\n",
      "Signal Significance @ L = 3000 fb^-1: 33.295\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 681.15 GeV\n",
      "0.0012179975159261188\n",
      "Maximal significance of 81.247 @ threshold = 0.9875738729243937\n",
      "(sig. maximized with tpr = 0.516, fpr = 0.001218)\n",
      "(new mass sensitivity of 950.12 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(xgradboost_350Gm, 350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 500$ GeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/xgboost/data.py:106: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:04:28] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "data50_500Gm_train, data50_500Gm_test = DA.gettraintest(1)\n",
    "xgradboost_500Gm = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, nthread=4, use_label_encoder=False)), \n",
    "    data50_500Gm_train, data50_500Gm_test);\n",
    "xgradboost_500Gm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.985; FPR: 0.0235546\n",
      "Signal Significance @ L = 3000 fb^-1: 17.457\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 749.73 GeV\n",
      "0.0002724468127729476\n",
      "Maximal significance of 56.436 @ threshold = 0.9965376428996667\n",
      "(sig. maximized with tpr = 0.505, fpr = 0.0002724)\n",
      "(new mass sensitivity of 1148.83 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(xgradboost_500Gm, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 1$ TeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/xgboost/data.py:106: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:55:23] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "data50_1Tm_train, data50_1Tm_test = DA.gettraintest(2)\n",
    "xgradboost_1Tm = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, nthread=4, use_label_encoder=False)), \n",
    "    data50_1Tm_train, data50_1Tm_test);\n",
    "xgradboost_1Tm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.995; FPR: 0.0050683\n",
      "Signal Significance @ L = 3000 fb^-1: 3.964\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 937.59 GeV\n",
      "8.01314155214552e-06\n",
      "Maximal significance of 24.382 @ threshold = 0.999731665495677\n",
      "(sig. maximized with tpr = 0.601, fpr = 8e-06)\n",
      "(new mass sensitivity of 1740.11 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(xgradboost_1Tm, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 2$ TeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/xgboost/data.py:106: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:44:28] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "data50_2Tm_train, data50_2Tm_test = DA.gettraintest(3)\n",
    "xgradboost_2Tm = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, nthread=4, use_label_encoder=False)), \n",
    "    data50_2Tm_train, data50_2Tm_test);\n",
    "xgradboost_2Tm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.997; FPR: 0.0010296\n",
      "Signal Significance @ L = 3000 fb^-1: 0.458\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 1158.71 GeV\n",
      "0.0\n",
      "Maximal significance of 7.181 @ threshold = 0.9999733388868765\n",
      "(sig. maximized with tpr = 0.845, fpr = 0.0)\n",
      "(new mass sensitivity of 2292.87 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(xgradboost_2Tm, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 4$ TeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/xgboost/data.py:106: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:18:45] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "data50_4Tm_train, data50_4Tm_test = DA.gettraintest(4)\n",
    "xgradboost_4Tm = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, nthread=4, use_label_encoder=False)), \n",
    "    data50_4Tm_train, data50_4Tm_test);\n",
    "xgradboost_4Tm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.995; FPR: 0.0015986\n",
      "Signal Significance @ L = 3000 fb^-1: 0.006\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 1094.42 GeV\n",
      "3.605913698465483e-05\n",
      "Maximal significance of 0.043 @ threshold = 0.990314042824841\n",
      "(sig. maximized with tpr = 0.986, fpr = 3.61e-05)\n",
      "(new mass sensitivity of 1686.71 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(xgradboost_4Tm, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(current_time)\n",
    "data50_4Tm_train, data50_4Tm_test = DA.gettraintest(4)\n",
    "xgradboost_4Tm = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, verbose=1, nthread=4, use_label_encoder=False)), \n",
    "    data50_4Tm_train, data50_4Tm_test);\n",
    "xgradboost_4Tm.fit()\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noised Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgradboost_350Gm_noise = refresh_model(xgradboost_350Gm_noise)\n",
    "xgradboost_500Gm_noise = refresh_model(xgradboost_500Gm_noise)\n",
    "xgradboost_1Tm_noise = refresh_model(xgradboost_1Tm_noise)\n",
    "xgradboost_2Tm_noise = refresh_model(xgradboost_2Tm_noise)\n",
    "xgradboost_4Tm_noise = refresh_model(xgradboost_4Tm_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/xgboost/data.py:106: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:45:34] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:45:49] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgradboost_350Gm_noise = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, verbose=1, nthread=4, use_label_encoder=False)), \n",
    "    data50_350Gm_train_noise, data50_350Gm_test_noise);\n",
    "xgradboost_350Gm_noise.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.962; FPR: 0.0623062\n",
      "Signal Significance @ L = 3000 fb^-1: 27.576\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 641.0 GeV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:69: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007372090227973877\n",
      "Maximal significance of 66.105 @ threshold = 0.99\n",
      "(sig. maximized with tpr = 0.331, fpr = 0.0007372)\n",
      "(new mass sensitivity of 898.72 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(xgradboost_350Gm_noise, 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/xgboost/data.py:106: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:26:50] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:27:04] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgradboost_500Gm_noise = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, verbose=1, nthread=4, use_label_encoder=False)), \n",
    "    data50_500Gm_train_noise, data50_500Gm_test_noise);\n",
    "xgradboost_500Gm_noise.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.98; FPR: 0.0311311\n",
      "Signal Significance @ L = 3000 fb^-1: 15.161\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 717.83 GeV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:69: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0011338595296285906\n",
      "Maximal significance of 43.823 @ threshold = 0.99\n",
      "(sig. maximized with tpr = 0.626, fpr = 0.0011339)\n",
      "(new mass sensitivity of 1011.38 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(xgradboost_500Gm_noise, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/xgboost/data.py:106: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:07:36] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:07:49] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgradboost_1Tm_noise = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, verbose=1, nthread=4, use_label_encoder=False)), \n",
    "    data50_1Tm_train_noise, data50_1Tm_test_noise);\n",
    "xgradboost_1Tm_noise.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.994; FPR: 0.0058776\n",
      "Signal Significance @ L = 3000 fb^-1: 3.682\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 918.37 GeV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:69: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006690973196041508\n",
      "Maximal significance of 9.927 @ threshold = 0.99\n",
      "(sig. maximized with tpr = 0.942, fpr = 0.0006691)\n",
      "(new mass sensitivity of 1205.37 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(xgradboost_1Tm_noise, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/xgboost/data.py:106: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:47:53] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:48:07] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgradboost_2Tm_noise = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, verbose=1, nthread=4, use_label_encoder=False)), \n",
    "    data50_2Tm_train_noise, data50_2Tm_test_noise);\n",
    "xgradboost_2Tm_noise.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.997; FPR: 0.0010737\n",
      "Signal Significance @ L = 3000 fb^-1: 0.448\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 1152.49 GeV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:69: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.214743589743589e-05\n",
      "Maximal significance of 1.494 @ threshold = 0.99\n",
      "(sig. maximized with tpr = 0.99, fpr = 9.21e-05)\n",
      "(new mass sensitivity of 1534.27 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(xgradboost_2Tm_noise, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/xgboost/data.py:106: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:29:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:29:53] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgradboost_4Tm_noise = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, verbose=1, nthread=4, use_label_encoder=False)), \n",
    "    data50_4Tm_train_noise, data50_4Tm_test_noise);\n",
    "xgradboost_4Tm_noise.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.994; FPR: 0.0016467\n",
      "Signal Significance @ L = 3000 fb^-1: 0.006\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 1090.07 GeV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:69: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.006570776072759e-05\n",
      "Maximal significance of 0.041 @ threshold = 0.99\n",
      "(sig. maximized with tpr = 0.985, fpr = 4.01e-05)\n",
      "(new mass sensitivity of 1669.29 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(xgradboost_4Tm_noise, 4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Background Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:11:02] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "data50_350Gm_train_bgdif, data50_350Gm_test_bgdif = DA.getidtraintest(0)\n",
    "xgradboost_350Gm_bgdif = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, nthread=4, use_label_encoder=False)), \n",
    "    data50_350Gm_train_bgdif, data50_350Gm_test_bgdif);\n",
    "xgradboost_350Gm_bgdif.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:51:47] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "data50_500Gm_train_bgdif, data50_500Gm_test_bgdif = DA.getidtraintest(1)\n",
    "xgradboost_500Gm_bgdif = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, nthread=4, use_label_encoder=False)), \n",
    "    data50_500Gm_train_bgdif, data50_500Gm_test_bgdif);\n",
    "xgradboost_500Gm_bgdif.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:31:17] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "data50_1Tm_train_bgdif, data50_1Tm_test_bgdif = DA.getidtraintest(2)\n",
    "xgradboost_1Tm_bgdif = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, nthread=4, use_label_encoder=False)), \n",
    "    data50_1Tm_train_bgdif, data50_1Tm_test_bgdif);\n",
    "xgradboost_1Tm_bgdif.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:08:20] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "data50_2Tm_train_bgdif, data50_2Tm_test_bgdif = DA.getidtraintest(3)\n",
    "xgradboost_2Tm_bgdif = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, nthread=4, use_label_encoder=False)), \n",
    "    data50_2Tm_train_bgdif, data50_2Tm_test_bgdif);\n",
    "xgradboost_2Tm_bgdif.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:43:54] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "data50_4Tm_train_bgdif, data50_4Tm_test_bgdif = DA.getidtraintest(4)\n",
    "xgradboost_4Tm_bgdif = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, nthread=4, use_label_encoder=False)), \n",
    "    data50_4Tm_train_bgdif, data50_4Tm_test_bgdif);\n",
    "xgradboost_4Tm_bgdif.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
