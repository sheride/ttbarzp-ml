{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unedited 47-dim data\n",
    "\n",
    "Initial training run-through: didn't go long enough, poor results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 350$ GeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.5957           71.48m\n",
      "         2           0.4411           70.42m\n",
      "         3           0.3686           68.21m\n",
      "         4           0.3194           66.41m\n",
      "         5           0.2932           64.68m\n",
      "         6           0.2807           63.01m\n",
      "         7           0.2674           61.40m\n",
      "         8           0.2561           59.97m\n",
      "         9           0.2495           58.47m\n",
      "        10           0.2461           56.95m\n",
      "        20           0.2103           42.56m\n",
      "        30           0.1997           28.33m\n",
      "        40           0.1918           14.16m\n",
      "        50           0.1863            0.00s\n"
     ]
    }
   ],
   "source": [
    "data50_350Gm_train, data50_350Gm_test = DA.gettraintest(0)\n",
    "gradboost_350Gm = sigbg_model(\n",
    "    make_pipeline(StandardScaler(), GradientBoostingClassifier(n_estimators=50, max_depth=7, learning_rate=1, verbose=1)), \n",
    "    data50_350Gm_train, data50_350Gm_test);\n",
    "gradboost_350Gm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.965; FPR: 0.034\n",
      "Signal Significance @ L = 3000 fb^-1: 37.056\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 704.99 GeV\n",
      "Maximal significance of 57.286 @ threshold = 0.975\n",
      "(sig. maximized with tpr = 0.772, fpr = 0.008)\n",
      "(new mass sensitivity of 814.82 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(gradboost_350Gm, 350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 500$ GeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.4478           68.88m\n",
      "         2           0.2957           67.04m\n",
      "         3           0.2330           65.72m\n",
      "         4           0.2001           64.47m\n",
      "         5           0.1800           63.25m\n",
      "         6           0.1657           61.89m\n",
      "         7           0.1599           60.53m\n",
      "         8           0.1527           59.17m\n",
      "         9           0.1483           57.83m\n",
      "        10           0.1437           56.47m\n",
      "        20           0.1251           42.47m\n",
      "        30           0.1185           28.44m\n",
      "        40           0.1142           14.21m\n",
      "        50           0.1141            0.00s\n"
     ]
    }
   ],
   "source": [
    "data50_500Gm_train, data50_500Gm_test = DA.gettraintest(1)\n",
    "gradboost_500Gm = sigbg_model(\n",
    "    make_pipeline(StandardScaler(), GradientBoostingClassifier(n_estimators=50, max_depth=7, learning_rate=1, verbose=1)), \n",
    "    data50_500Gm_train, data50_500Gm_test);\n",
    "gradboost_500Gm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.979; FPR: 0.021\n",
      "Signal Significance @ L = 3000 fb^-1: 18.355\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 761.37 GeV\n",
      "Maximal significance of 29.919 @ threshold = 0.9754\n",
      "(sig. maximized with tpr = 0.878, fpr = 0.006)\n",
      "(new mass sensitivity of 884.84 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(gradboost_500Gm, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 1$ TeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2971           68.75m\n",
      "         2           0.1358           67.62m\n",
      "         3           0.0825           66.43m\n",
      "         4           0.0630           65.12m\n",
      "         5           0.0541           63.72m\n",
      "         6           0.0494           62.38m\n",
      "         7           0.0467           61.01m\n",
      "         8           0.0448           59.59m\n",
      "         9           0.0427           58.14m\n",
      "        10           0.0417           56.78m\n",
      "        20           0.0362           42.64m\n",
      "        30           0.0364           28.42m\n",
      "        40           0.0509           14.22m\n",
      "        50           5.5445            0.00s\n"
     ]
    }
   ],
   "source": [
    "data50_1Tm_train, data50_1Tm_test = DA.gettraintest(2)\n",
    "gradboost_1Tm = sigbg_model(\n",
    "    make_pipeline(StandardScaler(), GradientBoostingClassifier(n_estimators=50, max_depth=7, learning_rate=1, verbose=1)), \n",
    "    data50_1Tm_train, data50_1Tm_test);\n",
    "gradboost_1Tm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.993; FPR: 0.007\n",
      "Signal Significance @ L = 3000 fb^-1: 3.373\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 895.94 GeV\n",
      "Maximal significance of 6.201 @ threshold = 0.9752\n",
      "(sig. maximized with tpr = 0.977, fpr = 0.002)\n",
      "(new mass sensitivity of 1060.74 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(gradboost_1Tm, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 2$ TeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2686           69.35m\n",
      "         2           0.1030           68.18m\n",
      "         3           0.0500           66.84m\n",
      "         4           0.0318           65.29m\n",
      "         5           0.0238           63.94m\n",
      "         6           0.0214           62.56m\n",
      "         7           0.0203           61.15m\n",
      "         8           0.0201           59.75m\n",
      "         9           0.0181           58.33m\n",
      "        10           0.0188           56.95m\n",
      "        20           0.6980           42.77m\n",
      "        30           0.6984           28.54m\n",
      "        40           0.6986           14.28m\n",
      "        50           0.9584            0.00s\n"
     ]
    }
   ],
   "source": [
    "data50_2Tm_train, data50_2Tm_test = DA.gettraintest(3)\n",
    "gradboost_2Tm = sigbg_model(\n",
    "    make_pipeline(StandardScaler(), GradientBoostingClassifier(n_estimators=50, max_depth=7, learning_rate=1, verbose=1)), \n",
    "    data50_2Tm_train, data50_2Tm_test);\n",
    "gradboost_2Tm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.996; FPR: 0.004\n",
      "Signal Significance @ L = 3000 fb^-1: 0.232\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 968.87 GeV\n",
      "Maximal significance of 0.708 @ threshold = 0.974\n",
      "(sig. maximized with tpr = 0.993, fpr = 0.0)\n",
      "(new mass sensitivity of 1291.19 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(gradboost_2Tm, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 4$ TeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2686           69.49m\n",
      "         2           0.1030           67.89m\n",
      "         3           0.0500           66.53m\n",
      "         4           0.0318           65.13m\n",
      "         5           0.0240           63.81m\n",
      "         6           0.0217           62.51m\n",
      "         7           0.0201           61.12m\n",
      "         8           0.0187           59.76m\n",
      "         9           0.0181           58.34m\n",
      "        10           0.0171           56.97m\n",
      "        20           0.1860           42.89m\n",
      "        30           0.1938           28.64m\n",
      "        40           0.2203           14.31m\n",
      "        50           0.2260            0.00s\n"
     ]
    }
   ],
   "source": [
    "data50_4Tm_train, data50_4Tm_test = DA.gettraintest(4) # accidentally had this at index = 3 initially\n",
    "gradboost_4Tm = sigbg_model(\n",
    "    make_pipeline(StandardScaler(), GradientBoostingClassifier(n_estimators=50, max_depth=7, learning_rate=1, verbose=1)), \n",
    "    data50_4Tm_train, data50_4Tm_test);\n",
    "gradboost_4Tm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.997; FPR: 0.003\n",
      "Signal Significance @ L = 3000 fb^-1: 0.005\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 1007.64 GeV\n",
      "Maximal significance of 0.01 @ threshold = 0.9743\n",
      "(sig. maximized with tpr = 0.994, fpr = 0.001)\n",
      "(new mass sensitivity of 1222.76 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(gradboost_4Tm, 4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unedited 47-dim data Pt 2\n",
    "\n",
    "Trained longer, used XGBClassifier which is much faster than pure scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 350$ GeV\n",
    "\n",
    "(test run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b644b03d3a5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata50_350Gm_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata50_350Gm_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgettraintest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m xgradboost_350Gm = sigbg_model(\n\u001b[1;32m      3\u001b[0m     make_pipeline(\n\u001b[1;32m      4\u001b[0m         \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         XGBClassifier(n_estimators=50, max_depth=7, learning_rate=1, verbose=1, nthread=4, use_label_encoder=False)), \n",
      "\u001b[0;31mNameError\u001b[0m: name 'DA' is not defined"
     ]
    }
   ],
   "source": [
    "data50_350Gm_train, data50_350Gm_test = DA.gettraintest(0)\n",
    "xgradboost_350Gm = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=50, max_depth=7, learning_rate=1, verbose=1, nthread=4, use_label_encoder=False)), \n",
    "    data50_350Gm_train, data50_350Gm_test);\n",
    "xgradboost_350Gm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.965; FPR: 0.035\n",
      "Signal Significance @ L = 3000 fb^-1: 36.548\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 701.87 GeV\n",
      "Maximal significance of 56.8 @ threshold = 0.9746\n",
      "(sig. maximized with tpr = 0.782, fpr = 0.009)\n",
      "(new mass sensitivity of 812.25 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(xgradboost_350Gm, 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 350$ GeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:09:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/xgboost/data.py:106: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:12:32] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:12:48] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "02:09:49\n"
     ]
    }
   ],
   "source": [
    "print(current_time)\n",
    "data50_350Gm_train, data50_350Gm_test = DA.gettraintest(0)\n",
    "xgradboost_350Gm = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, verbose=1, nthread=4, use_label_encoder=False)), \n",
    "    data50_350Gm_train, data50_350Gm_test);\n",
    "xgradboost_350Gm.fit()\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.975; FPR: 0.0434032\n",
      "Signal Significance @ L = 3000 fb^-1: 33.295\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 681.15 GeV\n",
      "0.0012179975159261188\n",
      "Maximal significance of 81.247 @ threshold = 0.9875738729243937\n",
      "(sig. maximized with tpr = 0.516, fpr = 0.001218)\n",
      "(new mass sensitivity of 950.12 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(xgradboost_350Gm, 350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 500$ GeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:09:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/xgboost/data.py:106: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:47:22] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:47:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "02:09:49\n"
     ]
    }
   ],
   "source": [
    "print(current_time)\n",
    "data50_500Gm_train, data50_500Gm_test = DA.gettraintest(1)\n",
    "xgradboost_500Gm = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, verbose=1, nthread=4, use_label_encoder=False)), \n",
    "    data50_500Gm_train, data50_500Gm_test);\n",
    "xgradboost_500Gm.fit()\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.985; FPR: 0.0235546\n",
      "Signal Significance @ L = 3000 fb^-1: 17.457\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 749.73 GeV\n",
      "0.0002724468127729476\n",
      "Maximal significance of 56.436 @ threshold = 0.9965376428996667\n",
      "(sig. maximized with tpr = 0.505, fpr = 0.0002724)\n",
      "(new mass sensitivity of 1148.83 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(xgradboost_500Gm, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 1$ TeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:09:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/xgboost/data.py:106: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:21:41] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:21:53] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "02:09:49\n"
     ]
    }
   ],
   "source": [
    "print(current_time)\n",
    "data50_1Tm_train, data50_1Tm_test = DA.gettraintest(2)\n",
    "xgradboost_1Tm = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, verbose=1, nthread=4, use_label_encoder=False)), \n",
    "    data50_1Tm_train, data50_1Tm_test);\n",
    "xgradboost_1Tm.fit()\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.995; FPR: 0.0050683\n",
      "Signal Significance @ L = 3000 fb^-1: 3.964\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 937.59 GeV\n",
      "8.01314155214552e-06\n",
      "Maximal significance of 24.382 @ threshold = 0.999731665495677\n",
      "(sig. maximized with tpr = 0.601, fpr = 8e-06)\n",
      "(new mass sensitivity of 1740.11 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(xgradboost_1Tm, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 2$ TeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:09:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/xgboost/data.py:106: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:56:08] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:56:20] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "02:09:49\n"
     ]
    }
   ],
   "source": [
    "print(current_time)\n",
    "data50_2Tm_train, data50_2Tm_test = DA.gettraintest(3)\n",
    "xgradboost_2Tm = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, verbose=1, nthread=4, use_label_encoder=False)), \n",
    "    data50_2Tm_train, data50_2Tm_test);\n",
    "xgradboost_2Tm.fit()\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.997; FPR: 0.0010296\n",
      "Signal Significance @ L = 3000 fb^-1: 0.458\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 1158.71 GeV\n",
      "0.0\n",
      "Maximal significance of 7.181 @ threshold = 0.9999733388868765\n",
      "(sig. maximized with tpr = 0.845, fpr = 0.0)\n",
      "(new mass sensitivity of 2292.87 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(xgradboost_2Tm, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $m_{Z'} = 4$ TeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:09:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/xgboost/data.py:106: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:28:47] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:28:59] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "02:09:49\n"
     ]
    }
   ],
   "source": [
    "print(current_time)\n",
    "data50_4Tm_train, data50_4Tm_test = DA.gettraintest(4)\n",
    "xgradboost_4Tm = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, verbose=1, nthread=4, use_label_encoder=False)), \n",
    "    data50_4Tm_train, data50_4Tm_test);\n",
    "xgradboost_4Tm.fit()\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.995; FPR: 0.0015986\n",
      "Signal Significance @ L = 3000 fb^-1: 0.006\n",
      "Mass sensitivity maximum @ L = 3000 fb^-1: 1094.42 GeV\n",
      "3.605913698465483e-05\n",
      "Maximal significance of 0.043 @ threshold = 0.990314042824841\n",
      "(sig. maximized with tpr = 0.986, fpr = 3.61e-05)\n",
      "(new mass sensitivity of 1686.71 GeV)\n"
     ]
    }
   ],
   "source": [
    "results(xgradboost_4Tm, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(current_time)\n",
    "data50_4Tm_train, data50_4Tm_test = DA.gettraintest(4)\n",
    "xgradboost_4Tm = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, verbose=1, nthread=4, use_label_encoder=False)), \n",
    "    data50_4Tm_train, data50_4Tm_test);\n",
    "xgradboost_4Tm.fit()\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noised Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/xgboost/data.py:106: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:22:06] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:22:21] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgradboost_350Gm_noise = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, verbose=1, nthread=4, use_label_encoder=False)), \n",
    "    data50_350Gm_train_noise, data50_350Gm_test_noise);\n",
    "xgradboost_350Gm_noise.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijahsheridan/opt/anaconda3/lib/python3.7/site-packages/xgboost/data.py:106: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:14:17] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:14:31] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgradboost_500Gm_noise = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, verbose=1, nthread=4, use_label_encoder=False)), \n",
    "    data50_500Gm_train_noise, data50_500Gm_test_noise);\n",
    "xgradboost_500Gm_noise.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgradboost_1Tm_noise = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, verbose=1, nthread=4, use_label_encoder=False)), \n",
    "    data50_1Tm_train_noise, data50_1Tm_test_noise);\n",
    "xgradboost_1Tm_noise.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgradboost_2Tm_noise = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, verbose=1, nthread=4, use_label_encoder=False)), \n",
    "    data50_2Tm_train_noise, data50_2Tm_test_noise);\n",
    "xgradboost_2Tm_noise.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgradboost_4Tm_noise = sigbg_model(\n",
    "    make_pipeline(\n",
    "        StandardScaler(), \n",
    "        XGBClassifier(n_estimators=250, max_depth=7, learning_rate=0.1, verbose=1, nthread=4, use_label_encoder=False)), \n",
    "    data50_4Tm_train_noise, data50_4Tm_test_noise);\n",
    "xgradboost_4Tm_noise.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
